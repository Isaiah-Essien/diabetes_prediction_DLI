{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaiah-Essien/diabetes_prediction_DLI/blob/main/new_diabetes_models_without_smote_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8HHnBRxBeYD"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install tensorflow==2.12.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeG_Gmj0TYCC"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.layers import Dense, Dropout, LeakyReLU\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Dense, Input, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJbaeHxQUBlq"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfkbqbbe9Oef"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "diabetes = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "# diabetes = pd.read_csv('/content/drive/My Drive/Deep Learning Indaba Submission/disease-data/diabetes_prediction_dataset.csv')  # Replace with your dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azbtC27zVEkJ"
      },
      "outputs": [],
      "source": [
        "# size of dataset\n",
        "diabetes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MViMAnpXPif"
      },
      "outputs": [],
      "source": [
        "# identify variable types\n",
        "diabetes.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boHTPT-_YdHt"
      },
      "outputs": [],
      "source": [
        "#Understand various summary statistics of the data\n",
        "diabetes.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp2zBia9doKC"
      },
      "outputs": [],
      "source": [
        "#Get count of values in a categorical variable\n",
        "diabetes['diabetes'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-VOnHQkfHqx"
      },
      "outputs": [],
      "source": [
        "diabetes['diabetes'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fRzMONPeXM1"
      },
      "outputs": [],
      "source": [
        "diabetes.age.hist(figsize=(10,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_7HMGKjhueE"
      },
      "outputs": [],
      "source": [
        "diabetes.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2XsSgcdWYA9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiVS2U6shBnU"
      },
      "outputs": [],
      "source": [
        "# # Scaling continues age because its a continues variable\n",
        "# scaler = MinMaxScaler()\n",
        "# diabetes[['age']] = scaler.fit_transform(diabetes[['age']])\n",
        "\n",
        "# diabetes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvm08zCZ0E5o"
      },
      "outputs": [],
      "source": [
        "diabetes['smoking_history'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esaTqhMqKFHe"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doVA1cK6Qb-p"
      },
      "outputs": [],
      "source": [
        "diabetes.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J19UhFgdKjLk"
      },
      "outputs": [],
      "source": [
        "diabetes = diabetes.replace({'smoking_history': {\"never\":1, \"not current\":2, \"former\":3, \"current\":4, \"ever\" : 5,  \"No Info\": 6 }})\n",
        "diabetes = diabetes.replace({'gender': {'Female': 1, 'Male': 2, 'Other': 0}})\n",
        "print(diabetes['smoking_history'].unique())\n",
        "print(diabetes['gender'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-mdXojWI63V"
      },
      "outputs": [],
      "source": [
        "Ydiab = diabetes['diabetes']\n",
        "Xdiab = diabetes.drop(['diabetes'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqQiZiTxln8C"
      },
      "source": [
        "### Function for Ploting training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxvlEW5sln8C"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(train_history, train, validation):\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Train History')\n",
        "    plt.ylabel(train)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['train', 'validation'], loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYUHMM-59fDl"
      },
      "outputs": [],
      "source": [
        "diabetes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrNoaUfjsHLt"
      },
      "outputs": [],
      "source": [
        "# Define features (X) and target (y)\n",
        "X = diabetes.drop(columns=['diabetes'])\n",
        "y = diabetes['diabetes']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
        "\n",
        "# # Apply SMOTE\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize the features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-nQBl9tuNvp"
      },
      "source": [
        "# Traditional ML Approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smxgY1Osu_76"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmzuewSKuNDI"
      },
      "outputs": [],
      "source": [
        "# Define and train the Logistic Regression model\n",
        "logistic_model = LogisticRegression(max_iter=1000)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create the confusion matrix display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ycE-QfPzyk8"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1uMTj35z7KK"
      },
      "outputs": [],
      "source": [
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf )\n",
        "\n",
        "# Create the confusion matrix display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdydV_Gs1quG"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pwX3RtV1wS3"
      },
      "outputs": [],
      "source": [
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "# Create the confusion matrix display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the Decision Tree model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9H4Scng4UyC"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwI7ptn_5DGv"
      },
      "outputs": [],
      "source": [
        "# Naive Bayes Model\n",
        "naive_bayes_model = GaussianNB()\n",
        "naive_bayes_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_nb = naive_bayes_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Naive Bayes model\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "\n",
        "# Create the confusion matrix display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_nb)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfdIMNG6tjlv"
      },
      "source": [
        "# Deep Lerning Approaches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGOVMc41CdW9"
      },
      "outputs": [],
      "source": [
        "# Convert resampled data to NumPy arrays if they are still DataFrames\n",
        "# Xd_resampled = np.array(Xd_resampled)\n",
        "# yd_resampled = np.array(yd_resampled)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    dmodel = Sequential()\n",
        "\n",
        "    # First layer\n",
        "    dmodel.add(Dense(512, input_dim=8, kernel_regularizer=l2(0.01)))\n",
        "    dmodel.add(BatchNormalization())\n",
        "    dmodel.add(LeakyReLU(alpha=0.1))\n",
        "    dmodel.add(Dropout(0.3))\n",
        "\n",
        "    # Hidden layers with decreasing units\n",
        "    dmodel.add(Dense(256, kernel_regularizer=l2(0.01)))\n",
        "    dmodel.add(BatchNormalization())\n",
        "    dmodel.add(LeakyReLU(alpha=0.1))\n",
        "    dmodel.add(Dropout(0.2))\n",
        "\n",
        "    dmodel.add(Dense(128, kernel_regularizer=l2(0.01)))\n",
        "    dmodel.add(BatchNormalization())\n",
        "    dmodel.add(LeakyReLU(alpha=0.1))\n",
        "    dmodel.add(Dropout(0.2))\n",
        "\n",
        "    dmodel.add(Dense(64, kernel_regularizer=l2(0.01)))\n",
        "    dmodel.add(BatchNormalization())\n",
        "    dmodel.add(LeakyReLU(alpha=0.1))\n",
        "    dmodel.add(Dropout(0.2))\n",
        "\n",
        "    dmodel.add(Dense(32, kernel_regularizer=l2(0.01)))\n",
        "    dmodel.add(BatchNormalization())\n",
        "    dmodel.add(LeakyReLU(alpha=0.1))\n",
        "    dmodel.add(Dropout(0.2))\n",
        "\n",
        "    dmodel.add(Dense(16, kernel_regularizer=l2(0.01)))\n",
        "    dmodel.add(BatchNormalization())\n",
        "    dmodel.add(LeakyReLU(alpha=0.1))\n",
        "    dmodel.add(Dropout(0.2))\n",
        "\n",
        "    # Final output layer\n",
        "    dmodel.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model with a custom learning rate\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    dmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return dmodel\n",
        "\n",
        "\n",
        "# 74 % I like this\n",
        "def create_residual_cnn(input_shape=(8, 1)):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # First Conv Block\n",
        "    x = Conv1D(filters=64, kernel_size=2, activation='relu', kernel_regularizer=l2(0.01), padding='same')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Second Conv Block with Residual Connection\n",
        "    y = Conv1D(filters=64, kernel_size=2, activation='relu', kernel_regularizer=l2(0.01), padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "    y = Conv1D(filters=64, kernel_size=2, activation='relu', kernel_regularizer=l2(0.01), padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "\n",
        "    # Match dimensions for residual connection\n",
        "    x_residual = Conv1D(filters=64, kernel_size=1, padding='same')(x)\n",
        "\n",
        "    y = Add()([x_residual, y])  # Residual connection\n",
        "\n",
        "    # Third Conv Block\n",
        "    z = Conv1D(filters=128, kernel_size=2, activation='relu', kernel_regularizer=l2(0.01), padding='same')(y)\n",
        "    z = BatchNormalization()(z)\n",
        "    z = Dropout(0.2)(z)\n",
        "\n",
        "    z = Flatten()(z)\n",
        "\n",
        "    # Dense layers\n",
        "    z = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(z)\n",
        "    z = BatchNormalization()(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "\n",
        "    z = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(z)\n",
        "    z = BatchNormalization()(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    cnn_model = Model(inputs=input_layer, outputs=output)\n",
        "    cnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return cnn_model\n",
        "\n",
        "# Initialize variables for cross-validation\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iU596HpB32m"
      },
      "source": [
        "# Residual CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCIvWUkH3R1N"
      },
      "outputs": [],
      "source": [
        "\n",
        "history_list_cnn = []\n",
        "confusion_matrices_cnn = []\n",
        "\n",
        "# Manual cross-validation\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    # X_train, X_test = Xd_resampled[train_index], Xd_resampled[test_index]\n",
        "    # y_train, y_test = yd_resampled[train_index], yd_resampled[test_index]\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_residual_cnn()\n",
        "\n",
        "    # Callbacks for early stopping and learning rate reduction\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        epochs=500,\n",
        "                        batch_size=32,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=0, callbacks=[early_stopping, reduce_lr])  # Set verbose to 0 to suppress output\n",
        "\n",
        "    # Store the history for plotting\n",
        "    history_list_cnn.append(history)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    confusion_matrices_cnn.append(cm)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, history in enumerate(history_list_cnn):\n",
        "    plt.plot(history.history['loss'], label=f'Train Loss Fold {i+1}')\n",
        "    plt.plot(history.history['val_loss'], label=f'Val Loss Fold {i+1}', linestyle='--')\n",
        "\n",
        "plt.title('Training and Validation Loss for Each Fold')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Display confusion matrices for each fold\n",
        "cm_plot_labels = ['No Diabetes', 'Diabetes']  # Replace with your actual labels\n",
        "\n",
        "for i, cm in enumerate(confusion_matrices_cnn):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cm_plot_labels)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for Fold {i + 1}')\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report for each fold\n",
        "    print(f'Classification Report for Fold {i + 1}:\\n')\n",
        "    print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzH3rarRCIYH"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jY86QWwCQF8"
      },
      "outputs": [],
      "source": [
        "# Define features (X) and target (y)\n",
        "X = diabetes.drop(columns=['diabetes'])\n",
        "y = diabetes['diabetes']\n",
        "\n",
        "history_list = []\n",
        "confusion_matrices = []\n",
        "\n",
        "# Manual cross-validation\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model()\n",
        "\n",
        "    # Callbacks for early stopping and learning rate reduction\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        epochs=500,\n",
        "                        batch_size=32,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=0, callbacks=[early_stopping, reduce_lr])  # Set verbose to 0 to suppress output\n",
        "\n",
        "    # Store the history for plotting\n",
        "    history_list.append(history)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    confusion_matrices.append(cm)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, history in enumerate(history_list):\n",
        "    plt.plot(history.history['loss'], label=f'Train Loss Fold {i+1}')\n",
        "    plt.plot(history.history['val_loss'], label=f'Val Loss Fold {i+1}', linestyle='--')\n",
        "\n",
        "plt.title('Training and Validation Loss for Each Fold')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Display confusion matrices for each fold\n",
        "cm_plot_labels = ['No Diabetes', 'Diabetes']  # Replace with your actual labels\n",
        "\n",
        "for i, cm in enumerate(confusion_matrices):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cm_plot_labels)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for Fold {i + 1}')\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report for each fold\n",
        "    print(f'Classification Report for Fold {i + 1}:\\n')\n",
        "    print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9JedR0NDx5p"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models_ = pd.read_csv(io.StringIO('''\n",
        "Model,Accuracy,Recall,F1_score\n",
        "Residual CNN,0.97,1.00,0.98\n",
        "Random Forest,0.97,1.00,0.98\n",
        "MLP,0.97,1.00,0.98\n",
        "Decision Tree,0.95,0.97,0.97\n",
        "Logistic Regression,0.96,0.99,0.98\n",
        "Naive Bayes,0.91,0.93,0.95\n",
        "'''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5A1GW47E2aG"
      },
      "outputs": [],
      "source": [
        "categories = ['Accuracy', 'Recall', 'F1_score']\n",
        "models = models_['Model']\n",
        "\n",
        "\n",
        "colors = ['#0096FF', '#D27D2D', '#808080']\n",
        "\n",
        "\n",
        "bar_width = 0.1\n",
        "index = np.arange(len(models))  # Positions for the groups of bars\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "\n",
        "for i, (category, color) in enumerate(zip(categories, colors)):\n",
        "    ax.bar(index + i * bar_width + i * 0.05, models_[category], bar_width, label=category, color=color)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Model Comparison: Accuracy, Recall, F1_score')\n",
        "ax.set_xticks(index + bar_width * (len(categories) - 1) / 2 + 0.05)\n",
        "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvm8fJaLE74o"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "ridge_model = Ridge(alpha=0.1)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "for feature, coef in zip(X.columns, ridge_model.coef_):\n",
        "    print(f\"{feature}: \\n{coef}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VnaBvgxE-Fb"
      },
      "outputs": [],
      "source": [
        "plt.bar(X.columns, ridge_model.coef_)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Coefficient Magnitude')\n",
        "plt.title('Feature Importance for Diabetes prediction based on Ridge Resgression')\n",
        "plt.xticks(rotation=90)\n",
        "plt.savefig('feature_importance_ridge.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis Testing for Feature Importance**"
      ],
      "metadata": {
        "id": "oF036C1HNXBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ozON4q4Fcnx"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "# Hypothesis 1: T-Test for Blood Glucose Level between Diabetes and No Diabetes\n",
        "# Null Hypothesis (H0): There is no significant difference in mean blood glucose levels between individuals with and without diabetes.\n",
        "# Alternative Hypothesis (H1): There is a significant difference in mean blood glucose levels between individuals with and without diabetes.\n",
        "\n",
        "# Separate the data\n",
        "group_with_diabetes = diabetes[diabetes['diabetes'] == 1]\n",
        "group_without_diabetes = diabetes[diabetes['diabetes'] == 0]\n",
        "\n",
        "# Perform the T-test\n",
        "t_stat, p_value = stats.ttest_ind(group_with_diabetes['blood_glucose_level'], group_without_diabetes['blood_glucose_level'])\n",
        "print(f\"T-test for Blood Glucose Level: T-statistic = {t_stat}, P-value = {p_value}\")\n",
        "\n",
        "# Hypothesis 2: Z-Test for Hypertension and Diabetes\n",
        "# Null Hypothesis (H0): Hypertension does not significantly affect the likelihood of having diabetes.\n",
        "# Alternative Hypothesis (H1): Hypertension significantly affects the likelihood of having diabetes.\n",
        "\n",
        "# Perform the Z-test\n",
        "z_stat, p_value = ztest(group_with_diabetes['hypertension'], group_without_diabetes['hypertension'])\n",
        "print(f\"Z-test for Hypertension: Z-statistic = {z_stat}, P-value = {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}